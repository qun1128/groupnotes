​                         2018年6月11日学习笔记

这个周主要看了一篇名为“Deep Residual Learning for Image Recognition”的论文。深度残差网络是2015年微软何凯明提出的深度卷积网络，一经出世，便在ImageNet中斩获图像分类、检测、定位三项的冠军。残差网络的特点是具有很深的网络结构。

一、提出问题：

首先论文提出的一个问题是对于神经网络来说，层数越多。网络表现的越好？答案是否定的。因为在训练深度网络时，要注意梯度弥散的问题。第二个问题就是层数越深，越暴露出一个问题，也就是退化问题：层数过深的平原网络具有更高的训练误差，图一所示训练层数为56层时训练误差和测试误差比训练层数为20时都要高。

![](https://ws1.sinaimg.cn/large/005QZeSZly1fs78g7tsffj30er07vq3o.jpg)

二、解决问题：

论文提出残差模块去解决上述的问题，残差模块就是在原始的平原网络上加了一条shortcut,输出就成为了一个恒定x加上两层网络的输出，也就是H(X)=F(X)+X， x是恒定的，所以拟合对象就变成F(X)，F(X)就是残差。拟合目标变为使F(X)趋近于0。

![](https://ws1.sinaimg.cn/large/005QZeSZly1fs78o4gehbj30a1061q32.jpg)



三、实验验证：

关于退化问题：论文在ImageNet 进行训练，所得结果如图所示。很明显看到右图加入残差模块之后，训练层数为34层要比训练层数为18层数误差要低。说明退化问题得到一定的解决。

![](https://ws1.sinaimg.cn/large/005QZeSZly1fs795rsbqaj30qw09igmx.jpg)

关于梯度弥散问题，论文从公式的角度证明,经过层层累加可得输出表达式为：
$$
X_{L}=X_{l}+\sum_{i=1}^{L-1}(F(X_{i}+W_{i}))
$$
再计算反向传播的时候，得到 loss function为：
$$
\frac{\partial\varepsilon}{\partial x_{l}}=\frac{\partial\varepsilon}{\partial x_{L}}\frac{\partial x_{L}}{\partial x_{l}}=\frac{\partial\varepsilon}{\partial x_{L}}(1+\frac{\partial}{\partial x_{L}}\sum_{i=1}^{L-1}(F(X_{i}+W_{i}))
$$
因为shortcut的加入使得gradient通过1流回任意浅层l，避免了经过weight layer造成的梯度弥散。

论文中考虑计算的成本，对残差块做了计算优化，即将两个3x3的卷积层替换为1x1 + 3x3 + 1x1, 如下图。新结构中的中间3x3的卷积层首先在一个降维1x1卷积层下减少了计算，然后在另一个1x1的卷积层下做了还原，既保持了精度又减少了计算量。

![](https://ws1.sinaimg.cn/large/005QZeSZly1fs7akhnx49j30em07f74q.jpg)

